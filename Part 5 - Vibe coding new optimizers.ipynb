{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKVaMUVdo-Rc"
      },
      "source": [
        "# Part 5:  Vibe-Coding Emerging Optimizers on CIFAR Classification\n",
        "\n",
        "As large language models (LLMs) reshape software engineering, we are entering a new era of AI-assisted programming where human creativity and machine execution merge. Instead of focusing on every low-level implementation detail, developers can prioritize **ideas, exploration, and iteration speed**, while the model handles much of the routine coding. This is the essence of [**vibe coding**](https://en.wikipedia.org/wiki/Vibe_coding?utm_source=chatgpt.com): coding by “vibes” and letting AI accelerate the path from concept to working system. In this homework, you will practice vibe coding by using AI assistants such as [ChatGPT](https://chat.openai.com), [Gemini](https://gemini.google.com/app), or [Claude](https://www.anthropic.com/claude-code) to help design and implement optimizers for CIFAR image classification.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. What is *Vibe Coding*?\n",
        "\n",
        "**Vibe coding** is a paradigm popularized by Andrej Karpathy in 2025. It emphasizes generating most of the code through LLMs while the human programmer acts as a **guide, tester, and refiner**. Instead of carefully reviewing every line, you iterate based on execution results, keeping the creative process at the center.  \n",
        "\n",
        "Karpathy put it simply:  \n",
        "> *“Fully giving in to the vibes, embracing exponentials, and forgetting that the code even exists.”*  \n",
        "\n",
        "**Learn more :**\n",
        "- [IBM: What is Vibe Coding?](https://www.ibm.com/think/topics/vibe-coding?utm_source=chatgpt.com)  \n",
        "- [Replit Blog: What is Vibe Coding?](https://blog.replit.com/what-is-vibe-coding?utm_source=chatgpt.com)  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Task Overview\n",
        "\n",
        "You will implement and compare four optimizers on **CIFAR-10 classification** with two architectures: a **Transformer**  and a **ResNet** .  \n",
        "\n",
        "- **Optimizers**: Muon , Scion , Dion , Adam (baseline)  \n",
        "- **Models**: Transformer, ResNet  \n",
        "- **Comparison metrics**: convergence speed, final test accuracy , training stability  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Optimizers to Implement\n",
        "\n",
        "### (1) Muon\n",
        "Muon applies **Newton–Schulz orthonormalization** to gradient updates of 2D weight matrices, making them invariant to input conditioning.  \n",
        "References:  \n",
        "- [Muon Blog (Keller Jordan)](https://kellerjordan.github.io/posts/muon/?utm_source=chatgpt.com)  \n",
        "- [Deriving Muon (Jeremy Bernstein)](https://jeremybernste.in/writing/deriving-muon?utm_source=chatgpt.com)  \n",
        "- [Muon GitHub Repo](https://github.com/KellerJordan/Muon?utm_source=chatgpt.com)  \n",
        "- [Convergence Bound (arXiv)](https://arxiv.org/abs/2507.01598?utm_source=chatgpt.com)  \n",
        "\n",
        "---\n",
        "\n",
        "### (2) Scion\n",
        "Scion constrains updates differently for hidden vs input/output layers, using **spectral norm** for hidden layers and **ℓ∞ norm** for others. This improves stability and hyperparameter transfer.  \n",
        "References:  \n",
        "- [Scion Paper](https://arxiv.org/abs/2502.07529)  \n",
        "- [Scion Official Code](https://github.com/LIONS-EPFL/scion)  \n",
        "\n",
        "---\n",
        "\n",
        "### (3) Dion\n",
        "Dion extends Muon-like orthonormal updates to **distributed training**. It reduces communication overhead while preserving synchronous semantics, making it efficient at large scale.  \n",
        "References:  \n",
        "- [Microsoft Research Blog](https://www.microsoft.com/en-us/research/blog/dion-the-distributed-orthonormal-update-revolution-is-here/?utm_source=chatgpt.com)  \n",
        "- [Dion Paper (arXiv)](https://arxiv.org/html/2504.05295v1?utm_source=chatgpt.com)  \n",
        "- [Dion GitHub Repo](https://github.com/microsoft/dion?utm_source=chatgpt.com)  \n",
        "\n",
        "---\n",
        "\n",
        "### (4) Adam\n",
        "Adam is the standard baseline optimizer combining momentum and adaptive learning rates. Use either `Adam` or `AdamW` from PyTorch.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Steps & Deliverables\n",
        "\n",
        "1. **Model Implementation (20 pts)**  \n",
        "   - Build a Transformer for CIFAR classification.  \n",
        "   - Build a ResNet (ResNet-18 or similar).  \n",
        "\n",
        "2. **Optimizer Integration (30 pts)**  \n",
        "   - Implement Muon, Scion, Dion optimizers using your AI assistant in a form which is compatible with `torch.optimizer`.  \n",
        "   - Use Adam as baseline.  \n",
        "\n",
        "3. **Training & Evaluation (30 pts)**  \n",
        "   - Train both models with all optimizers.  \n",
        "   - Collect metrics: training loss, validation accuracy, time-to-accuracy.  \n",
        "   - Present results with plots and a summary table.  \n",
        "\n",
        "4. **Discussion & Reflection (20 pts)**  \n",
        "   - Compare optimizers in terms of convergence speed, stability, and accuracy.  \n",
        "   - Reflect on your experience using **vibe coding** with AI assistants.  \n",
        "   - What worked well? What challenges did you face?  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Objectives\n",
        "\n",
        "- Understand and implement **novel optimizers** (Muon, Scion, Dion).  \n",
        "- Practice **vibe coding** as a workflow with LLMs.  \n",
        "- Compare optimizer performance on **CIFAR-10** across Transformer and ResNet architectures.  \n",
        "- Analyze results critically and reflect on the coding process.  \n",
        "\n",
        "---\n",
        "\n",
        "Good luck and enjoy vibe-coding your way through optimizers!!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xmsdJmAyo-Re"
      },
      "outputs": [],
      "source": [
        "#### Coding starts here.......\n",
        "import copy\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "from torchvision.transforms import v2 as T, transforms\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from torchvision.models import resnet18, vit_b_16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HjEw5Jyro-Rf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "cifar10_models.py\n",
        "\n",
        "Two PyTorch model wrappers for the assignment:\n",
        " - ResNetClassifier: wrapper around torchvision.models.resnet18\n",
        " - ViTClassifier: wrapper around torchvision.models.vit_b_16 (fallback-safe adjustments)\n",
        "\n",
        "IMPORTANT: Per your instruction, this file DOES NOT define or instantiate any optimizer.\n",
        "Plug in your professor's custom optimizer/algorithm when training.\n",
        "\n",
        "Usage:\n",
        "    from cifar10_models import ResNetClassifier, ViTClassifier\n",
        "    model = ResNetClassifier(num_classes=10, pretrained=False)\n",
        "    vit = ViTClassifier(num_classes=10, pretrained=False)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from typing import Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class ResNetClassifier(nn.Module):\n",
        "    \"\"\"Wrapper for torchvision.models.resnet18 with custom output classes.\n",
        "\n",
        "    Args:\n",
        "        num_classes: number of output classes (CIFAR-10 -> 10)\n",
        "        pretrained: whether to load ImageNet pretrained weights\n",
        "        in_channels: number of input channels (default 3). If different, a Conv layer is\n",
        "            prepended to adapt channels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 10, pretrained: bool = False, in_channels: int = 3):\n",
        "        super().__init__()\n",
        "        # load base resnet18\n",
        "        self.model = models.resnet18(pretrained=pretrained)\n",
        "\n",
        "        # adapt input channels if necessary\n",
        "        if in_channels != 3:\n",
        "            # replace the first conv layer to handle different input channel count\n",
        "            orig_conv = self.model.conv1\n",
        "            self.model.conv1 = nn.Conv2d(\n",
        "                in_channels,\n",
        "                orig_conv.out_channels,\n",
        "                kernel_size=orig_conv.kernel_size,\n",
        "                stride=orig_conv.stride,\n",
        "                padding=orig_conv.padding,\n",
        "                bias=orig_conv.bias is not None,\n",
        "            )\n",
        "\n",
        "        # replace the final fully-connected layer\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Input x shape: (B, C, H, W). For CIFAR-10, H=W=32 normally; if using pretrained weights\n",
        "        and larger resolution, consider upsampling or using appropriate transforms.\n",
        "        \"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class ViTClassifier(nn.Module):\n",
        "    \"\"\"Wrapper for a Vision Transformer from torchvision with adjustable head.\n",
        "\n",
        "    This class tries to be robust to different torchvision versions by checking common\n",
        "    attribute names for the classification head and replacing them with a new Linear.\n",
        "\n",
        "    Args:\n",
        "        num_classes: number of output classes\n",
        "        pretrained: whether to load ImageNet pretrained weights\n",
        "        image_size: expected input image size (used only for doc; transforms should match).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 10, pretrained: bool = False, image_size: int = 224):\n",
        "        super().__init__()\n",
        "        # create ViT from torchvision; function name depends on torchvision version\n",
        "        # We use vit_b_16 (base patch 16). If unavailable in your torchvision version, swap in\n",
        "        # the appropriate import or use timm.\n",
        "        try:\n",
        "            self.model = models.vit_b_16(pretrained=pretrained)\n",
        "        except Exception as e:\n",
        "            # Provide a helpful error message if user's environment doesn't have vit_b_16\n",
        "            raise RuntimeError(\n",
        "                \"Could not load torchvision.models.vit_b_16. Make sure torchvision >= 0.13 \"\n",
        "                \"or use an alternative ViT provider (e.g., timm). Original error: {}\".format(e)\n",
        "            )\n",
        "\n",
        "        # Now patch the classification head depending on attribute names\n",
        "        replaced = False\n",
        "        # Common structure: model.heads.head (torchvision >= 0.13+)\n",
        "        if hasattr(self.model, \"heads\") and hasattr(self.model.heads, \"head\"):\n",
        "            in_features = self.model.heads.head.in_features\n",
        "            self.model.heads.head = nn.Linear(in_features, num_classes)\n",
        "            replaced = True\n",
        "\n",
        "        # Some versions may have model.classifier or model.head\n",
        "        if not replaced:\n",
        "            if hasattr(self.model, \"classifier\") and isinstance(self.model.classifier, nn.Linear):\n",
        "                in_features = self.model.classifier.in_features\n",
        "                self.model.classifier = nn.Linear(in_features, num_classes)\n",
        "                replaced = True\n",
        "\n",
        "        if not replaced and hasattr(self.model, \"head\") and isinstance(self.model.head, nn.Linear):\n",
        "            in_features = self.model.head.in_features\n",
        "            self.model.head = nn.Linear(in_features, num_classes)\n",
        "            replaced = True\n",
        "\n",
        "        if not replaced:\n",
        "            # Last resort: attach a new attribute `head` and rely on forward passthrough.\n",
        "            # This may require editing the model.forward in extreme cases; raise a helpful error\n",
        "            raise RuntimeError(\n",
        "                \"Unable to locate a replaceable classification head on the loaded ViT model.\"\n",
        "                \" Please inspect the model structure or provide a torchvision version that supports vit_b_16.\"\n",
        "            )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Input x shape: (B, C, H, W). For torchvision ViT default pretrained weights, use H=W=224.\n",
        "        For CIFAR-10 (32x32) you may need to upsample in transforms or adapt patch embedding.\n",
        "        \"\"\"\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HsIVNDH8o-Rg"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\"\"\"\n",
        "CIFAR-10 training setup with custom Muon optimizer.\n",
        "\n",
        "This script:\n",
        "  - Loads CIFAR-10 DataLoaders (same configuration as before)\n",
        "  - Defines a custom Muon optimizer compatible with torch.optim\n",
        "  - Demonstrates how to train a model using it (you can plug in ResNetClassifier or ViTClassifier)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer, Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# ==============================\n",
        "# Custom Muon Optimizer\n",
        "# ==============================\n",
        "class Muon(Optimizer):\n",
        "    \"\"\"Muon optimizer (a momentum-based optimizer with normalized updates).\"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=0.01, momentum=0.9, weight_decay=0.0):\n",
        "        if lr <= 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "        super(Muon, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            momentum = group['momentum']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "\n",
        "                # Apply weight decay\n",
        "                if weight_decay != 0:\n",
        "                    grad = grad.add(p, alpha=weight_decay)\n",
        "\n",
        "                # Get state variables\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['velocity'] = torch.zeros_like(p)\n",
        "\n",
        "                velocity = state['velocity']\n",
        "\n",
        "                # Momentum update\n",
        "                velocity.mul_(momentum).add_(grad)\n",
        "\n",
        "                # Normalized update (Muon-specific idea)\n",
        "                norm_v = torch.norm(velocity)\n",
        "                norm_g = torch.norm(grad)\n",
        "                if norm_v > 0 and norm_g > 0:\n",
        "                    velocity.mul_(norm_g / norm_v)\n",
        "\n",
        "                p.add_(velocity, alpha=-lr)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ucqX2_wEo-Rg"
      },
      "outputs": [],
      "source": [
        "class Scion(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements Scion optimizer.\n",
        "\n",
        "    This is a conceptual optimizer based on the user's 'Muon' example.\n",
        "\n",
        "    The core idea of this version is to normalize the momentum-based\n",
        "    velocity vector to a unit vector, so the learning rate\n",
        "    directly controls the magnitude of the step in that direction.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=0.01, momentum=0.9, weight_decay=0.0, eps=1e-8):\n",
        "        if lr <= 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
        "        if eps < 0.0:\n",
        "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, eps=eps)\n",
        "        super(Scion, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            momentum = group['momentum']\n",
        "            weight_decay = group['weight_decay']\n",
        "            eps = group['eps']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "\n",
        "                # Apply weight decay\n",
        "                if weight_decay != 0:\n",
        "                    grad = grad.add(p, alpha=weight_decay)\n",
        "\n",
        "                # Get state variables\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['velocity'] = torch.zeros_like(p)\n",
        "\n",
        "                velocity = state['velocity']\n",
        "\n",
        "                # Momentum update\n",
        "                velocity.mul_(momentum).add_(grad)\n",
        "\n",
        "                # Normalized update (Scion-specific idea)\n",
        "                # We normalize the velocity vector to a unit vector.\n",
        "                # The learning rate then dictates the step size.\n",
        "\n",
        "                # addcdiv_ is an efficient way to do: p + alpha * (tensor1 / tensor2)\n",
        "                # Here: p = p + (-lr) * (velocity / (norm(velocity) + eps))\n",
        "                norm_v = torch.norm(velocity)\n",
        "\n",
        "                if norm_v > 0:\n",
        "                    p.addcdiv_(velocity, norm_v.add(eps), alpha=-lr)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dion(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements Dion optimizer.\n",
        "\n",
        "    This is a conceptual optimizer based on the user's 'Muon' example.\n",
        "\n",
        "    The core idea of this version is to scale the momentum-based\n",
        "    velocity by an exponential moving average of the *squared gradient norm*,\n",
        "    similar to RMSprop or Adam, but applied to the entire tensor's norm\n",
        "    rather than per-parameter.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=0.01, momentum=0.9, weight_decay=0.0, beta2=0.999, eps=1e-8):\n",
        "        if lr <= 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
        "        if beta2 < 0.0 or beta2 >= 1.0:\n",
        "            raise ValueError(f\"Invalid beta2 value: {beta2}\")\n",
        "        if eps < 0.0:\n",
        "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, beta2=beta2, eps=eps)\n",
        "        super(Dion, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            momentum = group['momentum']\n",
        "            weight_decay = group['weight_decay']\n",
        "            beta2 = group['beta2']\n",
        "            eps = group['eps']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "\n",
        "                # Apply weight decay\n",
        "                if weight_decay != 0:\n",
        "                    grad = grad.add(p, alpha=weight_decay)\n",
        "\n",
        "                # Get state variables\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['velocity'] = torch.zeros_like(p)\n",
        "                    # exp_avg_sq_norm is a scalar (tensor(0.))\n",
        "                    state['exp_avg_sq_norm'] = torch.zeros((), device=p.device)\n",
        "\n",
        "                velocity = state['velocity']\n",
        "                exp_avg_sq_norm = state['exp_avg_sq_norm']\n",
        "\n",
        "                # Momentum update\n",
        "                velocity.mul_(momentum).add_(grad)\n",
        "\n",
        "                # Get L2 norm of the current gradient, then square it\n",
        "                grad_norm_sq = torch.norm(grad).pow(2)\n",
        "\n",
        "                # Update exponential moving average of squared grad norms\n",
        "                exp_avg_sq_norm.mul_(beta2).add_(grad_norm_sq, alpha=1 - beta2)\n",
        "\n",
        "                # Normalized update (Dion-specific idea)\n",
        "                # We scale the velocity by the root-mean-square of past grad norms\n",
        "\n",
        "                # Denominator is sqrt(exp_avg_sq_norm) + eps\n",
        "                denom = exp_avg_sq_norm.sqrt().add(eps)\n",
        "\n",
        "                # p = p + (-lr) * (velocity / denom)\n",
        "                p.addcdiv_(velocity, denom, alpha=-lr)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "gRSXiB8yxT4A"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Data setup (same as before)\n",
        "# ==============================\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 4\n",
        "DATA_DIR = './data'\n",
        "\n",
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=train_transform)\n",
        "test_dataset = datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlJF5XA7xSiE",
        "outputId": "e7b9d638-e974-42e5-ced0-9f47f798664b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Example training function\n",
        "# ==============================\n",
        "def train_model(model, train_loader, test_loader, epochs=10, lr=0.01, algo=None):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if algo is None or algo == \"Adam\":\n",
        "      optimizer = Adam(model.parameters(), lr=lr)\n",
        "    elif algo == \"Muon\":\n",
        "      optimizer = Muon(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    elif algo == \"Scion\":\n",
        "      optimizer = Scion(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    elif algo == \"Dion\":\n",
        "      optimizer = Dion(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid algorithm: {algo}, please choose: Adam, Muon, Scion or Dion\")\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(epochs)):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_acc = 100. * correct / total\n",
        "        avg_loss = total_loss / total\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "# ==============================\n",
        "# Example usage (in notebook)\n",
        "# ==============================\n",
        "model = ResNetClassifier(num_classes=10, pretrained=False)\n",
        "train_model(model, train_loader, test_loader, epochs=10, lr=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzv5k-ayxM3M",
        "outputId": "f3b2fda2-da98-40ee-fe32-9b09b53a48ba"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [00:23<03:28, 23.22s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] | Loss: 1.8474 | Train Acc: 33.38%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 2/10 [00:46<03:08, 23.53s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10] | Loss: 1.4188 | Train Acc: 48.52%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 3/10 [01:09<02:42, 23.26s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10] | Loss: 1.1855 | Train Acc: 57.63%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 4/10 [01:32<02:16, 22.81s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10] | Loss: 1.0453 | Train Acc: 63.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [01:54<01:53, 22.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10] | Loss: 0.9476 | Train Acc: 66.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [02:17<01:31, 22.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10] | Loss: 0.8717 | Train Acc: 69.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [02:40<01:08, 22.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10] | Loss: 0.8085 | Train Acc: 71.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [03:02<00:45, 22.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10] | Loss: 0.7677 | Train Acc: 73.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [03:25<00:22, 22.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10] | Loss: 0.7187 | Train Acc: 75.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [03:48<00:00, 22.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10] | Loss: 0.6855 | Train Acc: 76.32%\n",
            "Training completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNetClassifier(num_classes=10, pretrained=False)\n",
        "train_model(model, train_loader, test_loader, epochs=10, lr=0.01, algo=\"Muon\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN0trtC7xjej",
        "outputId": "b57fa9c4-06fc-48ae-b0f6-23f70c1184c0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            " 10%|█         | 1/10 [00:27<04:07, 27.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] | Loss: 1.7193 | Train Acc: 36.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:55<03:41, 27.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10] | Loss: 1.4305 | Train Acc: 47.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [01:23<03:14, 27.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10] | Loss: 1.2987 | Train Acc: 53.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [01:50<02:44, 27.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10] | Loss: 1.1957 | Train Acc: 57.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [02:17<02:17, 27.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10] | Loss: 1.1171 | Train Acc: 59.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [02:45<01:50, 27.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10] | Loss: 1.0493 | Train Acc: 62.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [03:12<01:22, 27.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10] | Loss: 0.9972 | Train Acc: 64.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [03:39<00:54, 27.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10] | Loss: 0.9481 | Train Acc: 66.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [04:06<00:27, 27.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10] | Loss: 0.9099 | Train Acc: 67.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [04:33<00:00, 27.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10] | Loss: 0.8768 | Train Acc: 68.86%\n",
            "Training completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNetClassifier(num_classes=10, pretrained=False)\n",
        "train_model(model, train_loader, test_loader, epochs=10, lr=0.01, algo=\"Scion\")"
      ],
      "metadata": {
        "id": "gKWJgtwry_Gb",
        "outputId": "2df4c255-9917-4cc3-fea1-0732404976ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "addcdiv_() got an unexpected keyword argument 'alpha'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-706894800.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Scion\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4036829762.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, epochs, lr, algo)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m                             )\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2728593967.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnorm_v\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvelocity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: addcdiv_() got an unexpected keyword argument 'alpha'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTGz5qQZ0HKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}