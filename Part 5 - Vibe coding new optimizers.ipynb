{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5:  Vibe-Coding Emerging Optimizers on CIFAR Classification \n",
    "\n",
    "As large language models (LLMs) reshape software engineering, we are entering a new era of AI-assisted programming where human creativity and machine execution merge. Instead of focusing on every low-level implementation detail, developers can prioritize **ideas, exploration, and iteration speed**, while the model handles much of the routine coding. This is the essence of [**vibe coding**](https://en.wikipedia.org/wiki/Vibe_coding?utm_source=chatgpt.com): coding by “vibes” and letting AI accelerate the path from concept to working system. In this homework, you will practice vibe coding by using AI assistants such as [ChatGPT](https://chat.openai.com), [Gemini](https://gemini.google.com/app), or [Claude](https://www.anthropic.com/claude-code) to help design and implement optimizers for CIFAR image classification.  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is *Vibe Coding*? \n",
    "\n",
    "**Vibe coding** is a paradigm popularized by Andrej Karpathy in 2025. It emphasizes generating most of the code through LLMs while the human programmer acts as a **guide, tester, and refiner**. Instead of carefully reviewing every line, you iterate based on execution results, keeping the creative process at the center.  \n",
    "\n",
    "Karpathy put it simply:  \n",
    "> *“Fully giving in to the vibes, embracing exponentials, and forgetting that the code even exists.”*  \n",
    "\n",
    "**Learn more :**\n",
    "- [IBM: What is Vibe Coding?](https://www.ibm.com/think/topics/vibe-coding?utm_source=chatgpt.com)  \n",
    "- [Replit Blog: What is Vibe Coding?](https://blog.replit.com/what-is-vibe-coding?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Task Overview \n",
    "\n",
    "You will implement and compare four optimizers on **CIFAR-10 classification** with two architectures: a **Transformer**  and a **ResNet** .  \n",
    "\n",
    "- **Optimizers**: Muon , Scion , Dion , Adam (baseline)  \n",
    "- **Models**: Transformer, ResNet  \n",
    "- **Comparison metrics**: convergence speed, final test accuracy , training stability  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Optimizers to Implement \n",
    "\n",
    "### (1) Muon \n",
    "Muon applies **Newton–Schulz orthonormalization** to gradient updates of 2D weight matrices, making them invariant to input conditioning.  \n",
    "References:  \n",
    "- [Muon Blog (Keller Jordan)](https://kellerjordan.github.io/posts/muon/?utm_source=chatgpt.com)  \n",
    "- [Deriving Muon (Jeremy Bernstein)](https://jeremybernste.in/writing/deriving-muon?utm_source=chatgpt.com)  \n",
    "- [Muon GitHub Repo](https://github.com/KellerJordan/Muon?utm_source=chatgpt.com)  \n",
    "- [Convergence Bound (arXiv)](https://arxiv.org/abs/2507.01598?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "### (2) Scion \n",
    "Scion constrains updates differently for hidden vs input/output layers, using **spectral norm** for hidden layers and **ℓ∞ norm** for others. This improves stability and hyperparameter transfer.  \n",
    "References:  \n",
    "- [Scion Paper](https://arxiv.org/abs/2502.07529)  \n",
    "- [Scion Official Code](https://github.com/LIONS-EPFL/scion)  \n",
    "\n",
    "---\n",
    "\n",
    "### (3) Dion \n",
    "Dion extends Muon-like orthonormal updates to **distributed training**. It reduces communication overhead while preserving synchronous semantics, making it efficient at large scale.  \n",
    "References:  \n",
    "- [Microsoft Research Blog](https://www.microsoft.com/en-us/research/blog/dion-the-distributed-orthonormal-update-revolution-is-here/?utm_source=chatgpt.com)  \n",
    "- [Dion Paper (arXiv)](https://arxiv.org/html/2504.05295v1?utm_source=chatgpt.com)  \n",
    "- [Dion GitHub Repo](https://github.com/microsoft/dion?utm_source=chatgpt.com)  \n",
    "\n",
    "---\n",
    "\n",
    "### (4) Adam \n",
    "Adam is the standard baseline optimizer combining momentum and adaptive learning rates. Use either `Adam` or `AdamW` from PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Steps & Deliverables \n",
    "\n",
    "1. **Model Implementation (20 pts)**  \n",
    "   - Build a Transformer for CIFAR classification.  \n",
    "   - Build a ResNet (ResNet-18 or similar).  \n",
    "\n",
    "2. **Optimizer Integration (30 pts)**  \n",
    "   - Implement Muon, Scion, Dion optimizers using your AI assistant in a form which is compatible with `torch.optimizer`.  \n",
    "   - Use Adam as baseline.  \n",
    "\n",
    "3. **Training & Evaluation (30 pts)**  \n",
    "   - Train both models with all optimizers.  \n",
    "   - Collect metrics: training loss, validation accuracy, time-to-accuracy.  \n",
    "   - Present results with plots and a summary table.  \n",
    "\n",
    "4. **Discussion & Reflection (20 pts)**  \n",
    "   - Compare optimizers in terms of convergence speed, stability, and accuracy.  \n",
    "   - Reflect on your experience using **vibe coding** with AI assistants.  \n",
    "   - What worked well? What challenges did you face?  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Objectives \n",
    "\n",
    "- Understand and implement **novel optimizers** (Muon, Scion, Dion).  \n",
    "- Practice **vibe coding** as a workflow with LLMs.  \n",
    "- Compare optimizer performance on **CIFAR-10** across Transformer and ResNet architectures.  \n",
    "- Analyze results critically and reflect on the coding process.  \n",
    "\n",
    "---\n",
    "\n",
    "Good luck and enjoy vibe-coding your way through optimizers!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Coding starts here......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
