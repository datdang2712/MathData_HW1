{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 1\n",
        "\n",
        "#### EE-556 Mathematics of Data - Fall 2024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this homework, we consider a multiclass classification task modeled by multinomial (softmax) logistic regression. Your goal will be to analyze the estimator and its properties (convexity, existence/uniqueness), and to derive gradients/Hessians and smoothness bounds. The first part consists of theoretical questions only.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\">\n",
        "  ‚ÑπÔ∏è <strong>Information on group based work:</strong>\n",
        "</div>\n",
        "\n",
        "- You are to deliver only 1 notebook per group.\n",
        "- Asking assistance beyond your group is ok, but answers should be individual to the group.\n",
        "- In the event that there was <span style=\"color: red;\">disproportional work done</span> by different group members, let the TAs know.\n",
        "- Only one member of the group is allowed to use AI. We will require sharing the conversation history with the AI in the form of a public link. If you use multiple conversations across the same or multiple tools please share all of them. Name the person in your group who is allowed to use AI. We encourage you to use the AI to help you understand the material, but we ask you to write the code and theory solutions by yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 1px solid #f00; background-color: #fdd; padding: 10px; border-radius: 5px;\">\n",
        "  ‚ö†Ô∏è Do not forget: Write who are the people in your group as well as their respective SCIPER numbers\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Person 1 **Name**: Fiona Maria Jetzer || Person 1 **SCIPER**: 363927\n",
        "\n",
        "\n",
        "Person 2 **Name**: Duc Dat Dang || Person 2 **SCIPER**: 415129\n",
        "\n",
        "\n",
        "Person 3 **Name**: Pierre Mailler|| Person 3 **SCIPER**: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 1px solid #0a0; background-color: #dfd; padding: 10px; border-radius: 5px;\">\n",
        "  üìì Feedback on AI use: Please use the following cell to provide feedback on the AI use in this notebook.\n",
        "  \n",
        "  For example, how useful were the tools to you? Which tools did you use? Did you feel like they helped you understand the material better?\n",
        "</div"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Multiclass Softmax Logistic Regression - 15 Points\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now model multiclass classification with classes $c \\in \\{1,\\dots,C\\}$. For each sample $(\\mathbf{a}_i, b_i)$ with $\\mathbf{a}_i \\in \\mathbb{R}^p$ and $b_i \\in \\{1,\\dots,C\\}$, let $\\mathbf{X} = [\\mathbf{x}_1,\\dots,\\mathbf{x}_C] \\in \\mathbb{R}^{p\\times C}$ be the class weight matrix. The softmax model defines\n",
        "\n",
        "$$\n",
        "\\mathbb{P}(b_i = c \\mid \\mathbf{a}_i) = \\frac{\\exp(\\mathbf{a}_i^\\top \\mathbf{x}_c)}{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)}.\n",
        "$$\n",
        "\n",
        "Assume i.i.d. samples $\\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n$. Our goal is to estimate $\\mathbf{X}$ by maximum likelihood (and later with an $\\ell_2$ regularizer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(a)__ (1 point) Show that the negative log-likelihood $f$ can be written as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        " f(\\mathbf{X})\n",
        " &= - \\log \\mathbb{P}(b_1,\\dots,b_n\\mid \\mathbf{a}_1,\\dots,\\mathbf{a}_n)\\\\\n",
        " &= \\sum_{i=1}^n \\left[ -\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} + \\log \\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k) \\right].\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        " f(\\mathbf{X})\n",
        " &= - \\log \\mathbb{P}(b_1,\\dots,b_n\\mid \\mathbf{a}_1,\\dots,\\mathbf{a}_n)\\\\\n",
        " &= - \\log \\prod_{i=1}^{n} P(b_i \\mid a_i)\\\\\n",
        " &= \\sum_{i=1}^{n} - \\log  \\frac{\\exp(\\mathbf{a}_i^\\top \\mathbf{x}_{b_i})}{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k)}\\\\\n",
        " &= \\sum_{i=1}^{n} - \\log (\\exp(\\mathbf{a}_i^\\top \\mathbf{x}_{b_i})) + \\log \\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k) \\\\\n",
        " &= \\sum_{i=1}^n \\left[ -\\mathbf{a}_i^\\top \\mathbf{x}_{b_i} + \\log \\sum_{k=1}^C \\exp(\\mathbf{a}_i^\\top \\mathbf{x}_k) \\right].\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(b)__ (2 points) Show that $\\mathbf{u} \\mapsto \\log\\!\\left(\\sum_{k=1}^C e^{u_k}\\right)$ is convex on $\\mathbb{R}^C$. Then, show that $f(\\mathbf{X})$ is convex.\n",
        "\n",
        "\n",
        "Hint: use Jensen's inequality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let $u, v \\in \\mathbb{R}^C$ and $\\lambda \\in [0,1]$ and we note $X = [\\exp(\\lambda u_1) \\dots \\exp(\\lambda u_c)]$ and $Y=[\\exp((1-\\lambda) v_1) \\dots \\exp((1-\\lambda) v_c)]$,\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log\\left(\\sum_{k=1}^{C} \\exp(\\lambda u_k + (1-\\lambda) v_k)\\right)\n",
        "&= \\log\\left(\\sum_{k=1}^{C} \\exp(\\lambda u_k)\\exp((1-\\lambda) v_k)\\right) \\\\\n",
        "&= \\log(X Y^\\top)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "As $\\lambda + (1-\\lambda) = 1$, H√∂lder's inequality holds for $p=1/\\lambda$ and $q=1/(1-\\lambda)$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "X Y^\\top &\\leq \\|X\\|_p \\|Y\\|_q \\\\\n",
        "\\log(X Y^\\top) &\\leq \\log(\\|X\\|_p \\|Y\\|_q) \\\\\n",
        "&\\leq \\log\\left(\\left[\\sum_{k=1}^{C} \\exp(\\lambda u_k)^{1/\\lambda}\\right]^\\lambda \\left[\\sum_{k=1}^{C} \\exp((1-\\lambda) v_k)^{1/(1-\\lambda)}\\right]^{1-\\lambda}\\right) \\\\\n",
        "&\\leq \\lambda \\log\\left(\\sum_{k=1}^{C} \\exp(u_k)\\right) + (1-\\lambda) \\log\\left(\\sum_{k=1}^{C} \\exp(v_k)\\right)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Thus, we proved:\n",
        "$$\n",
        "\\log\\left(\\sum_{k=1}^{C} \\exp(\\lambda u_k + (1-\\lambda) v_k)\\right) \\leq \\lambda \\log\\left(\\sum_{k=1}^{C} \\exp(u_k)\\right) + (1-\\lambda) \\log\\left(\\sum_{k=1}^{C} \\exp(v_k)\\right)\n",
        "$$\n",
        "\n",
        "Which means that $u \\mapsto \\log\\left(\\sum_{k=1}^{C} \\exp(u_k)\\right)$ is convex by Jensen's inequality definition.\n",
        "\n",
        "Now, Let's prove that $ f $ is convex.\n",
        "\n",
        "Let $ x, y \\in \\mathbb{R}^{pxC} $, $ x \\in [0, 1] $.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f(\\lambda x + (1-\\lambda)y) &= \\sum_{i=1}^n -a_i^T (\\lambda x_{b_i} + (1-\\lambda)y_{b_i}) + \\log \\sum_{k=1}^c \\exp \\{ a_i^T (\\lambda x_k + (1-\\lambda)y_k) \\} \\\\\n",
        "&\\leq \\sum_{i=1}^n (-\\lambda a_i^T x_{b_i} - (1-\\lambda)a_i^T y_{b_i}) + \\lambda \\log \\sum_{k=1}^c e^{a_i^T x_k} + (1-\\lambda) \\log \\sum_{k=1}^c e^{a_i^T y_k} \\\\\n",
        "&\\leq \\sum_{i=1}^n \\left[ \\lambda (-a_i^T x_{b_i} + \\log \\sum_{k=1}^c e^{a_i^T x_k}) + (1-\\lambda) (-a_i^T y_{b_i} + \\log \\sum_{k=1}^c e^{a_i^T y_k}) \\right] \\\\\n",
        "&\\leq \\lambda \\sum_{i=1}^n (-a_i^T x_{b_i} + \\log \\sum_{k=1}^c e^{a_i^T x_k}) + (1-\\lambda) \\sum_{i=1}^n (-a_i^T y_{b_i} + \\log \\sum_{k=1}^c e^{a_i^T y_k}) \\\\\n",
        "&\\leq \\lambda f(x) + (1-\\lambda) f(y)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Thus, we proved that $ f $ is convex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You have just established that the negative log-likelihood is a convex function. So in principle, any local minimum of the maximum likelihood estimator\n",
        "$$\n",
        "\\mathbf{X}^\\star_{ML} = \\arg\\min_{\\mathbf{X} \\in \\mathbb{R}^{p\\times C}} f(\\mathbf{X})\n",
        "$$\n",
        "\n",
        "is a global minimum. But does the minimum always exist? We will ponder this question in the following three points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(c)__ (1 point) Explain the difference between infima and minima. Give an example of a convex function on $\\mathbb{R}$ that does not attain its infimum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Infimum ($ \\inf f $): Greatest lower bound of function $ f $\n",
        "- $ \\inf f = \\inf \\{ f(x) : x \\in \\text{dom(f)} \\} $\n",
        "- May not be attained by the function\n",
        "- Always exists for bounded functions\n",
        "\n",
        "Minimum ($ \\min f $): Smallest value actually attained\n",
        "- $ \\min f = f(x_0) $ for some $ x_0 $ in domain\n",
        "- Requires $ \\exists x_0 $ such that $ f(x_0) = \\inf f $\n",
        "\n",
        "Key difference:\n",
        "- $ \\inf f $ is the \"best possible\" lower bound\n",
        "- $ \\min f $ is the \"actual lowest value\" attained\n",
        "- $ \\min f $ exists $ \\Rightarrow \\min f = \\inf f $\n",
        "- But $ \\inf f $ can exist without $ \\min f $\n",
        "\n",
        "An example of a convex function on $\\mathbb{R}$ that does not attain its infimum is $g(x) = \\exp(x)$.\n",
        "\n",
        "We have g convex because $g''(x) = \\exp(x) \\geq 0$ and $g$ does not attain its infimum as $ \\lim_{x \\to -\\infty} g(x) = 0 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(d)__ (1 point) Assume there exists $\\mathbf{X}_0 \\in \\mathbb{R}^{p\\times C}$ such that for all $i$,\n",
        "$$\n",
        "\\mathbf{a}_i^\\top \\mathbf{x}_{0, b_i} - \\max_{k \\neq b_i} \\mathbf{a}_i^\\top \\mathbf{x}_{0,k} > 0.\n",
        "$$\n",
        "This is called one-versus-all complete separation in multiclass settings. Give a geometric interpretation (e.g., for $p=2$) and explain why the name is appropriate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The condition means that for each observation $i$, the score of its true class $b_i$ is strictly greater than the maximum score of all other classes.\n",
        "\n",
        "Each $\\mathbf{x}_{0,k}$ represents a \"prototype\" or \"weight vector\" for class $k$ in $\\mathbb{R}^2$.\n",
        "The dot product $\\mathbf{a}_i^\\top \\mathbf{x}_{0,k}$ measures the alignment between the observation $\\mathbf{a}_i$ and the class prototype $k$.\n",
        "The condition means that each point $\\mathbf{a}_i$ is closer (in terms of directional alignment) to the prototype of its true class than to any other prototype\n",
        "\n",
        "This terminology is appropriate because:\n",
        "\n",
        "\"Complete Separation\": The classes are perfectly separable - there exists a linear classifier that can correctly classify all observations without any errors.\n",
        "\n",
        "\"One-versus-All\": For each class $k$, we can find a hyperplane that perfectly separates the points of class $k$ from all points of the other classes combined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this, you should see that it is likely that some datasets satisfy the complete separation assumption. Unfortunately, as you will show next, this can become an obstacle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(e)__ (1 point) In a one-versus-all complete separation setting (as in (d)), prove that $f$ does not attain its minimum. Hint: consider $f(\\alpha \\mathbf{X}_0)$ as $\\alpha \\to +\\infty$ and compare it to $f(\\mathbf{X}_0)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plan of the proof: We want to prove that $f$ does not attain its minimum. To do so, we are going to show that $f(\\alpha X_0) \\to 0$ when $\\alpha \\to +\\infty$. As $f(X) \\geq 0$ for all $X$, it means that $\\inf f(X) = 0$. Then we will conclude that $f$ does not attain its minimum by showing that $f(X) > 0$.\n",
        "\n",
        "1. Let's show that $f(\\alpha X_0) \\to 0$\n",
        "\n",
        "\\begin{align*}\n",
        "f(\\alpha X_0) &= \\sum_{i=1}^n \\left[ -a_i^T (\\alpha x_{0,b_i}) + \\log \\sum_{k=1}^C \\exp(a_i^T (\\alpha x_{0,k})) \\right] \\\\\n",
        "&= \\sum_{i=1}^n \\left[ -a_i^T (\\alpha x_{0,b_i}) + \\log \\left( \\exp(a_i^T (\\alpha x_{0,b_i})) + \\sum_{k \\neq b_i} \\exp(a_i^T (\\alpha x_{0,k})) \\right) \\right] \\\\\n",
        "&= \\sum_{i=1}^n \\left[ -a_i^T (\\alpha x_{0,b_i}) + a_i^T (\\alpha x_{0,b_i}) + \\log \\left( 1 + \\sum_{k \\neq b_i} \\exp(a_i^T (\\alpha x_{0,k}) - a_i^T (\\alpha x_{0,b_i})) \\right) \\right] \\\\\n",
        "&= \\sum_{i=1}^n \\log \\left( 1 + \\sum_{k \\neq b_i} \\exp(\\alpha (a_i^T x_{0,k} - a_i^T x_{0,b_i})) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "We have: $\\forall i,\\ \\mathbf{a}_i^\\top \\mathbf{x}_{0, b_i} - \\max_{k \\neq b_i} \\mathbf{a}_i^\\top \\mathbf{x}_{0,k} > 0$\n",
        "\n",
        "We can deduce that: $\\forall k \\neq b_i, \\forall i,\\ \\mathbf{a}_i^\\top \\mathbf{x}_{0,k} - \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i} < 0$\n",
        "\n",
        "This implies: $\\forall k \\neq b_i,\\ \\exp(\\alpha(\\mathbf{a}_i^\\top \\mathbf{x}_{0,k} - \\mathbf{a}_i^\\top \\mathbf{x}_{0,b_i})) \\to 0$ for $\\alpha \\to \\infty$\n",
        "\n",
        "Thus: $\\lim_{\\alpha \\to \\infty} f(\\alpha X_0) = \\sum_{i=1}^n \\log(1 + 0) = 0$\n",
        "\n",
        "As $\\inf_X f(X) \\leq \\lim_{\\alpha \\to \\infty} f(\\alpha X_0)$, we have $\\inf_X f(X) \\leq 0$\n",
        "\n",
        "2. Now let's show that $f(X) > 0$\n",
        "\n",
        "\\begin{align*}\n",
        "f(X) &= \\sum_{i=1}^n \\left[ -a_i^T x_{b_i} + \\log \\sum_{k=1}^C \\exp(a_i^T x_k) \\right]\n",
        "\\end{align*}\n",
        "\n",
        "It holds that $\\sum_{k=1}^C \\exp(a_i^T x_k) > \\exp(a_i^T x_{b_i})$ for all $i$\n",
        "\n",
        "Therefore:\n",
        "\\begin{align*}\n",
        "f(X) &> \\sum_{i=1}^n \\left[ -a_i^T x_{b_i} + \\log (\\exp(a_i^T x_{b_i})) \\right] \\\\\n",
        "&= \\sum_{i=1}^n \\left[ -a_i^T x_{b_i} + a_i^T x_{b_i} \\right] \\\\\n",
        "&= 0\n",
        "\\end{align*}\n",
        "\n",
        "So $f(X) > 0$ for all $X$.\n",
        "\n",
        "3. Conclusion:\n",
        "\n",
        "We have $f(X) > 0$ for all $X$ and $\\inf_X f(X) \\leq 0$, which proves that $f$ does not attain its minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We resolve this issue by adding a regularizer. Consider the regularized function\n",
        "\n",
        "$$\n",
        " f_\\mu(\\mathbf{X}) = f(\\mathbf{X}) + \\frac{\\mu}{2} \\|\\mathbf{X}\\|_F^2, \\quad \\mu > 0.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(f)__ (1 point) Show that the gradient with respect to $\\mathbf{X}$ of $f_\\mu$ can be expressed as\n",
        "$$\n",
        " \\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n \\mathbf{a}_i \\big( \\mathbf{p}_i - \\mathbf{e}_{b_i} \\big)^\\top + \\mu \\mathbf{X},\\tag{1}\n",
        "$$\n",
        "where $\\mathbf{e}_{b_i} \\in \\mathbb{R}^C$ is the [one-hot vector](https://en.wikipedia.org/wiki/One-hot) for class $b_i$, $\\mathbf{p}_i \\in \\mathbb{R}^C$ has entries $p_{i,c} = \\mathbb{P}(b_i=c\\mid \\mathbf{a}_i)$ under the softmax model, and $\\mathbf{a}_i(\\mathbf{p}_i - \\mathbf{e}_{b_i})^\\top$ denotes the outer product.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have\n",
        "$$\n",
        "\\nabla_{\\mathbf{X}}f_\\mu(\\mathbf{X}) = \\nabla_{\\mathbf{X}} f(\\mathbf{X}) + \\nabla_{\\mathbf{X}} \\left( \\frac{\\mu}{2} \\|\\mathbf{X}\\|_F^2 \\right)\n",
        "$$\n",
        "\n",
        "With:\n",
        "$$\n",
        "\\nabla_{\\mathbf{X}} \\left( \\frac{\\mu}{2} \\|\\mathbf{X}\\|_F^2 \\right) = \\mu \\mathbf{X}\n",
        "$$\n",
        "\n",
        "And:\n",
        "\\begin{align*}\n",
        "\\nabla_{\\mathbf{X}} f(\\mathbf{X}) &= \\nabla_{\\mathbf{X}} \\left( \\sum_{i=1}^n - \\mathbf{a}_i^T \\mathbf{x}_{b_i} + \\log \\sum_{k=1}^C \\exp(\\mathbf{a}_i^T \\mathbf{x}_k) \\right) \\\\\n",
        "&= \\sum_{i=1}^n \\nabla_{\\mathbf{X}} (-\\mathbf{a}_i^T \\mathbf{x}_{b_i}) + \\nabla_{\\mathbf{X}} \\left( \\log \\sum_{k=1}^C \\exp(\\mathbf{a}_i^T \\mathbf{x}_k) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "We have:\n",
        "$$\n",
        "\\nabla_{\\mathbf{X}} (-\\mathbf{a}_i^T \\mathbf{x}_{b_i}) = -\\mathbf{a}_i \\mathbf{e}_{b_i}^T\n",
        "$$\n",
        "\n",
        "and for $j = 1, \\ldots, C$:\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\mathbf{x}_j} \\left( \\log \\sum_{k=1}^C \\exp(\\mathbf{a}_i^T \\mathbf{x}_k) \\right) = \\frac{\\mathbf{a}_i \\exp(\\mathbf{a}_i^T \\mathbf{x}_j)}{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^T \\mathbf{x}_k)}\n",
        "$$\n",
        "\n",
        "We deduce that:\n",
        "$$\n",
        "\\nabla_{\\mathbf{X}} \\left( \\log \\sum_{k=1}^C \\exp(\\mathbf{a}_i^T \\mathbf{x}_k) \\right) = \n",
        "\\begin{bmatrix}\n",
        "\\mathbf{a}_i \\mathbb{P}(b_i = 1 | \\mathbf{a}_i) &\n",
        "\\cdots &\n",
        "\\mathbf{a}_i \\mathbb{P}(b_i = j | \\mathbf{a}_i) &\n",
        "\\cdots &\n",
        "\\mathbf{a}_i \\mathbb{P}(b_i = C | \\mathbf{a}_i)\n",
        "\\end{bmatrix} = \\mathbf{a}_i \\mathbf{p}_i^T\n",
        "$$\n",
        "\n",
        "We conclude that:\n",
        "\\begin{align*}\n",
        "\\nabla_{\\mathbf{X}} f(\\mathbf{X}) &= \\sum_{i=1}^n -\\mathbf{a}_i \\mathbf{e}_{b_i}^T + \\mathbf{a}_i \\mathbf{p}_i^T \\\\\n",
        "&= \\sum_{i=1}^n \\mathbf{a}_i (\\mathbf{p}_i - \\mathbf{e}_{b_i})^T\n",
        "\\end{align*}\n",
        "\n",
        "We finally wrap it together to show:\n",
        "$$\n",
        "\\nabla_{\\mathbf{X}} f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n \\mathbf{a}_i (\\mathbf{p}_i - \\mathbf{e}_{b_i})^T + \\mu \\mathbf{X}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(g)__ (1 point) Show that the Hessian of $f_\\mu$ can be written as\n",
        "$$\n",
        " \\nabla^2 f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top) \\otimes \\big( \\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top \\big) + \\mu \\mathbf{I},\\tag{2}\n",
        "$$\n",
        "where $\\otimes$ is the Kronecker product, and $\\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top$ is the softmax Jacobian, which is positive semidefinite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have :\n",
        "\n",
        "\\begin{aligned}\n",
        "\\nabla^2 f_\\mu(\\mathbf{X}) \n",
        "&= \\nabla \\left( \\sum_{i=1}^n \\mathbf{a}_i (\\mathbf{p}_i - \\mathbf{e}_{b_i})^T + \\mu \\mathbf{X} \\right) \\\\\n",
        "&= \\sum_{i=1}^n \\nabla \\left[ \\mathbf{a}_i (\\mathbf{p}_i - \\mathbf{e}_{b_i})^T \\right] + \\nabla (\\mu \\mathbf{X}) \\\\\n",
        "&= \\sum_{i=1}^n \\nabla \\left[ \\mathbf{a}_i (\\mathbf{p}_i - \\mathbf{e}_{b_i})^T \\right] + \\mu \\mathbf{I}\n",
        "\\end{aligned}\n",
        "\n",
        "Let's now focus on: $\\nabla \\left[ \\mathbf{a}_i (\\mathbf{p}_i - \\mathbf{e}_{b_i})^T \\right]$\n",
        "\n",
        "For the $(l,j)$-th component:\n",
        "$$\n",
        "\\left( \\nabla \\left[ \\mathbf{a}_i (\\mathbf{p}_i - \\mathbf{e}_{b_i})^T \\right] \\right)_{l,j} = \\mathbf{a}_i \\mathbf{a}_i^T \\left( \\mathbb{\\delta}_l^j \\mathbf{p}_{i,l} - \\mathbf{p}_{i,l} \\mathbf{p}_{i,j} \\right)\n",
        "$$\n",
        "\n",
        "With $\\mathbf{p}_i$ defined as:\n",
        "$$\n",
        "\\mathbf{p}_i = \\begin{bmatrix}\n",
        "\\mathbb{P}(b_i = 1 | \\mathbf{a}_i) \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbb{P}(b_i = C | \\mathbf{a}_i)\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "\\displaystyle\\frac{\\exp(\\mathbf{a}_i^T \\mathbf{x}_1)}{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^T \\mathbf{x}_k)} \\\\\n",
        "\\vdots \\\\\n",
        "\\displaystyle\\frac{\\exp(\\mathbf{a}_i^T \\mathbf{x}_C)}{\\sum_{k=1}^C \\exp(\\mathbf{a}_i^T \\mathbf{x}_k)}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We conclude that:\n",
        "$$\n",
        "\\nabla \\left[ \\mathbf{a}_i (\\mathbf{p}_i - \\mathbf{e}_{b_i})^T \\right] = (\\mathbf{a}_i \\mathbf{a}_i^\\top) \\otimes \\big( \\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i \\mathbf{p}_i^\\top \\big)\n",
        "$$\n",
        "\n",
        "We wrap it together to find that:\n",
        "$$\n",
        "\\nabla^2 f_\\mu(\\mathbf{X}) = \\sum_{i=1}^n (\\mathbf{a}_i \\mathbf{a}_i^\\top) \\otimes \\big( \\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i \\mathbf{p}_i^\\top \\big) + \\mu \\mathbf{I}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(h)__ (1 point) Show that $f_\\mu$ is $\\mu$-strongly convex.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We know that $\\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top$ is positive semidefinite.\n",
        "And $a_i a_i^T = \\|a_i\\|_{2}^2 \\geq 0$\n",
        "\n",
        "We deduce that $\\sum_{i=1}^n (\\mathbf{a}_i \\mathbf{a}_i^\\top) \\otimes \\big( \\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i \\mathbf{p}_i^\\top \\big) \\geq 0$\n",
        "\n",
        "We conclude that $\\nabla^2 f_\\mu(\\mathbf{X}) \\geq \\mu \\mathbf I$\n",
        "Which means by definition that $f_\\mu$ is $\\mu$-strongly convex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(i)__ (1 point) Is it possible for a strongly convex function to not attain its minimum? Justify your reasoning (you may assume the domain is $\\mathbb{R}^{p\\times C}$).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No, it is impossible for a strongly convex function to not attain its minimum.\n",
        "\n",
        "Let $f$ be a twice continuously differentiable $\\mu$-strongly convex function. Then $\\nabla ^2 f \\geq \\mu \\mathbf I > 0$.\n",
        "This implies that $\\nabla f$ is strictly increasing. \n",
        "\n",
        "As $f$ is strongly convex, we know that $f$ is coercive. Which means that $\\lim_{\\|\\mathbf{X}\\| \\to \\infty} f(\\mathbf{X}) = +\\infty$.\n",
        "\n",
        "As $f$ is coercive, we now know that there exist $x,y \\in dom(f)$ such that $\\nabla f(x)<0$ and $\\nabla f(y)>0$.\n",
        "\n",
        "By continuity, there exist $x^*$ such that $\\nabla f(x^*) = 0$\n",
        "\n",
        "As $f$ is convex and coercive, we know that $x^*$ is the unique and global minimum of the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now show that $f_\\mu$ is smooth, i.e., $\\nabla f_\\mu$ is L-Lipschitz with respect to the Frobenius norm, with a simple conservative bound\n",
        "$$\n",
        " L = \\|\\mathbf{A}\\|_F^2 + \\mu.\n",
        "$$\n",
        "where\n",
        "$$\n",
        " \\mathbf{A} = \\begin{bmatrix}\n",
        "  \\leftarrow &  \\mathbf{a}_1^\\top & \\rightarrow \\\\\n",
        "  \\leftarrow &  \\mathbf{a}_2^\\top & \\rightarrow \\\\\n",
        "   &  \\ldots &  \\\\\n",
        "  \\leftarrow &  \\mathbf{a}_n^\\top & \\rightarrow \\\\\n",
        " \\end{bmatrix}.\n",
        "$$\n",
        "(You may use that the operator norm of the softmax Jacobian is bounded by 1/4, and a looser bound $\\le 1$ is acceptable for grading.)\n",
        "\n",
        "Hint: check the properties of the spectral norm with respect to dot product, Kronecker product, and outer product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(1 point for all three questions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-1)__ Show that $\\lambda_{\\max}(\\mathbf{a}_i\\mathbf{a}_i^T) = \\left\\| \\mathbf{a}_i\\right\\|_2^2$, where $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For $a_i \\in \\mathbb R^p$, $trace(\\mathbf{a}_i \\mathbf{a}_i^T) = \\sum_{k=1}^p \\mathbf{a}_{i,k}^2 =  \\|\\mathbf{a}_i\\|_2^2 = \\sum_{k=1}^R \\lambda_{k}$, where $R = rank(\\mathbf{a}_i \\mathbf{a}_i^T)$\n",
        "\n",
        "Since $a_i a_i^T$ is just linear combination of each columns with number from $\\mathbf{a}_i^T, rank(\\mathbf{a}_i \\mathbf{a}_i^T) = 1$. Hence, $a_i a_i^T$ only has one eigenvalue.\n",
        "\n",
        "$\\lambda_{\\text max}(\\mathbf{a}_i \\mathbf{a}_i^T) = \\sum_{k=1}^p \\mathbf{a}_{i,k}^2 = \\|\\mathbf{a}_i\\|_2^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-2)__ Using (2), show that $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{X})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "\\lambda_{\\text max} (\\nabla ^2 f_\\mu (x))\n",
        "&= \\lambda_{\\text max} (\\sum_{i=1}^n (\\mathbf{a}_i\\mathbf{a}_i^\\top) \\otimes \\big( \\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{p}_i\\mathbf{p}_i^\\top \\big) + \\mu \\mathbf{I}) \\\\\n",
        "&= \\lambda_{\\text max} (\\sum_{i=1}^n \\mathbf{a}_i\\mathbf{a}_i^\\top \\otimes \\operatorname{Diag}(\\mathbf{p}_i) - \\mathbf{a}_i\\mathbf{a}_i^\\top \\otimes \\mathbf{p}_i\\mathbf{p}_i^\\top) + \\mu \\\\\n",
        "&\\leq \\mu + \\sum_{i=1}^n \\lambda_{\\text max} (\\mathbf{a}_i\\mathbf{a}_i^\\top \\otimes \\operatorname{Diag}(\\mathbf{p}_i)) - \\lambda_{\\text max} (\\mathbf{a}_i\\mathbf{a}_i^\\top \\otimes \\mathbf{p}_i\\mathbf{p}_i^\\top) \\\\\n",
        "&\\leq \\mu + \\sum_{i=1}^n \\lambda_{\\text max} (\\mathbf{a}_i\\mathbf{a}_i^\\top \\otimes \\operatorname{Diag}(\\mathbf{p}_i)) \\\\\n",
        "&\\leq \\mu + \\sum_{i=1}^n \\lambda_{\\text max} (\\mathbf{a}_i\\mathbf{a}_i^\\top \\otimes \\mathbf I ) \\\\\n",
        "&\\leq \\mu + \\sum_{i=1}^n \\lambda_{\\text max} (\\mathbf{a}_i\\mathbf{a}_i^\\top) \\\\\n",
        "&\\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(j-3)__ Conclude that $f_\\mu$ is $L$-smooth for $L = \\|\\mathbf{A}\\|_F^2 + \\mu$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define $h(x) = \\frac L 2 \\|X\\|_2^2 - f(X)$\n",
        "\n",
        "We deduce that $\\nabla h(X) = LX - \\nabla f(X)$ and $ \\nabla^2 h(X) = L\\mathbf I - \\nabla^2 f(X)$\n",
        "\n",
        "We proved that : $\\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{X})) \\leq \\sum_{i=1}^{n} \\|\\mathbf{a}_i\\|_2^2 + \\mu = \\mu + \\|A\\|_F$\n",
        "\n",
        "Then : $\\|A\\|_F + \\mu - \\lambda_{\\max}(\\nabla^2 f_\\mu(\\mathbf{X})) \\geq 0$\n",
        "\n",
        "This concludes that for all $\\lambda \\in Spectre\\{L\\mathbf I -\\nabla^2 f(X) \\}$, we have $\\lambda \\geq 0$\n",
        "\n",
        "We deduce that $L\\mathbf I -\\nabla^2 f(X) \\succeq 0$, then $\\nabla^2 h(X) \\succeq 0$\n",
        "\n",
        "This proves that $h$ is convex and that $f$ is L-smooth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__(l)__ (1 point) KL divergence and NLL. Let $q(b_i\\mid\\mathbf{a}_i)$ be the true label distribution and $p(b_i\\mid\\mathbf{a}_i)$ the model softmax. Write the KL divergence $\\mathrm{KL}(q\\,\\|\\,p)$ and show that minimizing the KL divergence between $q$ and $p$ is equivalent to minimizing the negative log-likelihood derived in (a).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "D_{KL} (q||p)\n",
        "&= \\sum_{i=1}^n \\sum_{j=1}^C q(b_j | a_i) \\log(\\frac{q(b_j | a_i)}{p(b_j|a_i)}) \\\\\n",
        "&= \\sum_{i=1}^n \\sum_{j=1}^C q(b_j | a_i) (\\log(q(b_j | a_i)) - \\log(p(b_j|a_i))) \\\\\n",
        "&= \\sum_{i=1}^n \\sum_{j=1}^C q(b_j | a_i) \\log(q(b_j | a_i)) - \\sum_{i=1}^n \\sum_{j=1}^C q(b_j | a_i) \\log(p(b_j|a_i)) \\\\\n",
        "&= \\sum_{i=1}^n \\sum_{j=1}^C q(b_j | a_i) \\log(q(b_j | a_i)) - \\sum_{i=1}^n \\sum_{j=1}^C q(b_j | a_i) \\log(\\frac{e^{a_i^Tx_j}}{\\sum_{k=1}^C e^{a_i^Tx_k}}) \\\\\n",
        "&= \\sum_{i=1}^n \\sum_{j=1}^C q(b_j | a_i) \\log(q(b_j | a_i)) + \\sum_{i=1}^n \\sum_{j=1}^C q(b_j | a_i)  (-a_i^Tx_j + \\log{\\sum_{k=1}^C e^{a_i^Tx_k}}) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Knowing that the first term doesnt depend on X we can write :\n",
        "$$\n",
        "\\begin{aligned}\n",
        "min_{X\\in\\mathbb{R}^{pxC}} D_{KL}(q||p) \n",
        "&= min_{X\\in\\mathbb{R}^{pxC}} \\sum_{i=1}^n \\sum_{j=1}^C q(b_j | a_i)  (-a_i^Tx_j + \\log{\\sum_{k=1}^C e^{a_i^Tx_k}}) \\\\\n",
        "&= min_{X\\in\\mathbb{R}^{pxC}} \\sum_{i=1}^n -a_i^Tx_{b_i} + \\log{\\sum_{k=1}^C e^{a_i^Tx_k}} \\\\\n",
        "&= min_{X\\in\\mathbb{R}^{pxC}} f(X)\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From your work in this section, you have shown that the maximum likelihood estimator for multiclass softmax logistic regression might not exist, but it can be guaranteed to exist by adding a $\\|\\cdot\\|_F^2$ regularizer. Consequently, the estimator for $\\mathbf{X}$ we will use will be the solution of the smooth strongly convex problem,\n",
        "$$\n",
        " \\mathbf{X}^\\star = \\arg\\min_{\\mathbf{X} \\in \\mathbb{R}^{p\\times C}} f(\\mathbf{X}) + \\frac{\\mu}{2}\\|\\mathbf{X}\\|_F^2.\\tag{3}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binary logistic regression (specialization for Part 2)\n",
        "\n",
        "While this part analyzed the multiclass (softmax) setting, in the next exercise we will continue under the simplified two-class case.\n",
        "\n",
        "Let labels be $b_i \\in \\{-1, +1\\}$, features $\\mathbf{a}_i \\in \\mathbb{R}^p$, and weight vector $\\mathbf{x} \\in \\mathbb{R}^p$. Define the sigmoid\n",
        "$$\n",
        "\\sigma(t) = \\frac{1}{1+e^{-t}}.\n",
        "$$\n",
        "Model the conditional distribution as\n",
        "$$\n",
        "\\mathbb{P}(b_i = j \\mid \\mathbf{a}_i) = \\sigma\\big(j\\, \\mathbf{a}_i^\\top \\mathbf{x}\\big), \\quad j \\in \\{-1,+1\\}.\n",
        "$$\n",
        "The likelihood over i.i.d. samples $\\{(\\mathbf{a}_i, b_i)\\}_{i=1}^n$ is\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}) = \\prod_{i=1}^n \\sigma\\big(b_i\\, \\mathbf{a}_i^\\top \\mathbf{x}\\big),\n",
        "$$\n",
        "so the negative log-likelihood is\n",
        "$$\n",
        " f(\\mathbf{x}) = -\\log \\mathcal{L}(\\mathbf{x}) = \\sum_{i=1}^n \\log\\big(1 + e^{-b_i\\, \\mathbf{a}_i^\\top \\mathbf{x}}\\big).\n",
        "$$\n",
        "\n",
        "__(m)__ (1 point) Show that the gradient of the negative log-likelihood is the standard binary logistic regression gradient:\n",
        "$$\n",
        "\\nabla f(\\mathbf{x}) = \\sum_{i=1}^n \\big(-b_i\\, \\sigma(-b_i\\, \\mathbf{a}_i^\\top \\mathbf{x})\\big)\\, \\mathbf{a}_i.\n",
        "$$\n",
        "(Hint: use the chain rule and $\\sigma'(t) = \\sigma(t)\\big(1-\\sigma(t)\\big)$.)\n",
        "\n",
        "We will use this binary formulation in Part 2 - First order methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "\\nabla f(x) \n",
        "&= \\nabla (\\sum_{i=1}^n log(1+e^{-b_ia_i^Tx})) \\\\\n",
        "&= \\sum_{i=1}^n \\nabla (log(1+e^{-b_ia_i^Tx})) \\\\\n",
        "&= \\sum_{i=1}^n \\frac{-a_ib_ie^{-b_ia_i^Tx}}{1 + e^{-b_ia_i^Tx}} \\\\\n",
        "&= \\sum_{i=1}^n \\frac{-a_ib_i}{1 + e^{b_ia_i^Tx}}\\\\\n",
        "&= \\sum_{i=1}^n -b_i \\sigma (-b_ia_i^Tx)a_i\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We conclude that : $\\nabla f(X) = \\sum_{i=1}^n -b_i \\sigma (-b_ia_i^Tx)a_i$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
